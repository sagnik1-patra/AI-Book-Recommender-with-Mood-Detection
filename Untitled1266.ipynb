{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "560343d4-bdff-4dfb-8527-fb129264997b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loading BX files...\n",
      "[INFO] Loading preprocessed side-info...\n",
      "[INFO] Building user/book encoders and CSR interactions...\n",
      "[OK] Wrote PKL → C:\\Users\\sagni\\Downloads\\AI Book Recommender with Mood Detection\\bx_artifacts.pkl\n",
      "[OK] Wrote H5  → C:\\Users\\sagni\\Downloads\\AI Book Recommender with Mood Detection\\bx_interactions.h5\n",
      "[OK] Wrote YAML → C:\\Users\\sagni\\Downloads\\AI Book Recommender with Mood Detection\\bx_config.yaml\n",
      "[OK] Wrote JSON → C:\\Users\\sagni\\Downloads\\AI Book Recommender with Mood Detection\\bx_summary.json\n",
      "\n",
      "All artifacts created successfully ✅\n",
      "Users: 92,108 | Items: 270,170 | Ratings: 1,031,190 | Density: 0.004144%\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# Book-Crossing + Preprocessed book side-info → Artifacts (PKL/H5/YAML/JSON)\n",
    "# INPUT (your exact paths):\n",
    "#   C:\\Users\\sagni\\Downloads\\archive\\Books Data with Category Language and Summary\\Preprocessed_data.csv\n",
    "#   C:\\Users\\sagni\\Downloads\\archive\\Book reviews\\Book reviews\\BX_Books.csv\n",
    "#   C:\\Users\\sagni\\Downloads\\archive\\Book reviews\\Book reviews\\BX-Book-Ratings.csv\n",
    "#   C:\\Users\\sagni\\Downloads\\archive\\Book reviews\\Book reviews\\BX-Users.csv\n",
    "#\n",
    "# OUTPUT (artifacts):\n",
    "#   C:\\Users\\sagni\\Downloads\\AI Book Recommender with Mood Detection\\bx_artifacts.pkl\n",
    "#   C:\\Users\\sagni\\Downloads\\AI Book Recommender with Mood Detection\\bx_interactions.h5\n",
    "#   C:\\Users\\sagni\\Downloads\\AI Book Recommender with Mood Detection\\bx_config.yaml\n",
    "#   C:\\Users\\sagni\\Downloads\\AI Book Recommender with Mood Detection\\bx_summary.json\n",
    "\n",
    "import os, sys, json, pickle, time, platform, re, warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import h5py\n",
    "import yaml\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "# -----------------------\n",
    "# Paths (edit if needed)\n",
    "# -----------------------\n",
    "BASE_DIR      = r\"C:\\Users\\sagni\\Downloads\\AI Book Recommender with Mood Detection\"\n",
    "PP_PATH       = r\"C:\\Users\\sagni\\Downloads\\archive\\Books Data with Category Language and Summary\\Preprocessed_data.csv\"\n",
    "BX_BOOKS      = r\"C:\\Users\\sagni\\Downloads\\archive\\Book reviews\\Book reviews\\BX_Books.csv\"\n",
    "BX_RATINGS    = r\"C:\\Users\\sagni\\Downloads\\archive\\Book reviews\\Book reviews\\BX-Book-Ratings.csv\"\n",
    "BX_USERS      = r\"C:\\Users\\sagni\\Downloads\\archive\\Book reviews\\Book reviews\\BX-Users.csv\"\n",
    "\n",
    "PKL_OUT       = str(Path(BASE_DIR) / \"bx_artifacts.pkl\")\n",
    "H5_OUT        = str(Path(BASE_DIR) / \"bx_interactions.h5\")\n",
    "YAML_OUT      = str(Path(BASE_DIR) / \"bx_config.yaml\")\n",
    "JSON_OUT      = str(Path(BASE_DIR) / \"bx_summary.json\")\n",
    "\n",
    "Path(BASE_DIR).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# -----------------------\n",
    "# Helpers\n",
    "# -----------------------\n",
    "def read_csv_robust(path, **kw):\n",
    "    \"\"\"\n",
    "    Try common encodings for BX_* files, which often need latin-1.\n",
    "    \"\"\"\n",
    "    encodings = [\"latin-1\", \"cp1252\", \"utf-8\", \"utf-8-sig\"]\n",
    "    last_err = None\n",
    "    for enc in encodings:\n",
    "        try:\n",
    "            return pd.read_csv(path, encoding=enc, **kw)\n",
    "        except Exception as e:\n",
    "            last_err = e\n",
    "    raise last_err\n",
    "\n",
    "def norm_isbn(s):\n",
    "    \"\"\"\n",
    "    Normalize ISBN as string: strip, remove spaces, keep leading zeros.\n",
    "    \"\"\"\n",
    "    if pd.isna(s):\n",
    "        return \"\"\n",
    "    s = str(s).strip()\n",
    "    s = s.replace(\" \", \"\")\n",
    "    return s\n",
    "\n",
    "def clean_text(s):\n",
    "    if pd.isna(s):\n",
    "        return \"\"\n",
    "    s = str(s).strip()\n",
    "    s = re.sub(r\"\\s+\", \" \", s)\n",
    "    return s\n",
    "\n",
    "# -----------------------\n",
    "# Load BX datasets\n",
    "# -----------------------\n",
    "if not Path(BX_BOOKS).exists() or not Path(BX_RATINGS).exists() or not Path(BX_USERS).exists():\n",
    "    raise SystemExit(\"Missing BX files. Check BX_BOOKS, BX_RATINGS, BX_USERS paths.\")\n",
    "\n",
    "print(\"[INFO] Loading BX files...\")\n",
    "books = read_csv_robust(BX_BOOKS, sep=\";\", on_bad_lines=\"skip\", low_memory=False)\n",
    "ratings = read_csv_robust(BX_RATINGS, sep=\";\", on_bad_lines=\"skip\", low_memory=False)\n",
    "users = read_csv_robust(BX_USERS, sep=\";\", on_bad_lines=\"skip\", low_memory=False)\n",
    "\n",
    "# Standardize BX columns\n",
    "# Common BX schema:\n",
    "# - BX-Books: ISBN; Book-Title; Book-Author; Year-Of-Publication; Publisher; Image-URL-S; Image-URL-M; Image-URL-L\n",
    "# - BX-Book-Ratings: User-ID; ISBN; Book-Rating (0-10; 0 = implicit)\n",
    "# - BX-Users: User-ID; Location; Age\n",
    "books.columns = [c.strip() for c in books.columns]\n",
    "ratings.columns = [c.strip() for c in ratings.columns]\n",
    "users.columns = [c.strip() for c in users.columns]\n",
    "\n",
    "# Keep important columns and clean\n",
    "books[\"ISBN\"] = books[\"ISBN\"].map(norm_isbn) if \"ISBN\" in books.columns else \"\"\n",
    "if \"Book-Title\" in books.columns:\n",
    "    books[\"Book-Title\"] = books[\"Book-Title\"].map(clean_text)\n",
    "else:\n",
    "    books[\"Book-Title\"] = \"\"\n",
    "\n",
    "if \"Book-Author\" in books.columns:\n",
    "    books[\"Book-Author\"] = books[\"Book-Author\"].map(clean_text)\n",
    "else:\n",
    "    books[\"Book-Author\"] = \"\"\n",
    "\n",
    "if \"Year-Of-Publication\" in books.columns:\n",
    "    # sometimes messy; coerce numeric\n",
    "    books[\"Year-Of-Publication\"] = pd.to_numeric(books[\"Year-Of-Publication\"], errors=\"coerce\")\n",
    "\n",
    "# Ratings: coerce types\n",
    "if \"User-ID\" not in ratings.columns or \"ISBN\" not in ratings.columns or \"Book-Rating\" not in ratings.columns:\n",
    "    raise SystemExit(\"BX-Book-Ratings.csv is missing required columns (User-ID, ISBN, Book-Rating).\")\n",
    "\n",
    "ratings[\"User-ID\"] = ratings[\"User-ID\"].astype(str)\n",
    "ratings[\"ISBN\"] = ratings[\"ISBN\"].map(norm_isbn)\n",
    "ratings[\"Book-Rating\"] = pd.to_numeric(ratings[\"Book-Rating\"], errors=\"coerce\").fillna(0).astype(int)\n",
    "\n",
    "# Users\n",
    "if \"User-ID\" in users.columns:\n",
    "    users[\"User-ID\"] = users[\"User-ID\"].astype(str)\n",
    "else:\n",
    "    users[\"User-ID\"] = \"\"\n",
    "\n",
    "# -----------------------\n",
    "# Load Preprocessed side-info\n",
    "# -----------------------\n",
    "if not Path(PP_PATH).exists():\n",
    "    warnings.warn(f\"Preprocessed_data.csv NOT found at: {PP_PATH}. Proceeding without side-info.\")\n",
    "    pp = pd.DataFrame(columns=[\"ISBN\",\"Title\",\"Category\",\"Language\",\"Summary\"])\n",
    "else:\n",
    "    print(\"[INFO] Loading preprocessed side-info...\")\n",
    "    # this file is usually UTF-8/UTF-16; try robustly\n",
    "    try:\n",
    "        pp = read_csv_robust(PP_PATH, low_memory=False)\n",
    "    except Exception:\n",
    "        # last resort: sep=',' default\n",
    "        pp = pd.read_csv(PP_PATH, low_memory=False)\n",
    "\n",
    "    # Normalize likely columns\n",
    "    pp_cols = {c.lower().strip(): c for c in pp.columns}\n",
    "    # Try to identify columns\n",
    "    isbn_col = None\n",
    "    for k in pp_cols:\n",
    "        if k in (\"isbn\",\"book_isbn\"):\n",
    "            isbn_col = pp_cols[k]; break\n",
    "    title_col = None\n",
    "    for k in pp_cols:\n",
    "        if k in (\"title\",\"book_title\",\"book-name\",\"name\"):\n",
    "            title_col = pp_cols[k]; break\n",
    "    cat_col = None\n",
    "    for k in pp_cols:\n",
    "        if \"category\" in k:\n",
    "            cat_col = pp_cols[k]; break\n",
    "    lang_col = None\n",
    "    for k in pp_cols:\n",
    "        if \"lang\" in k:\n",
    "            lang_col = pp_cols[k]; break\n",
    "    sum_col = None\n",
    "    for k in pp_cols:\n",
    "        if \"summary\" in k or \"desc\" in k or \"synopsis\" in k:\n",
    "            sum_col = pp_cols[k]; break\n",
    "\n",
    "    # Build a clean side-info frame\n",
    "    pp2 = pd.DataFrame()\n",
    "    if isbn_col:\n",
    "        pp2[\"ISBN\"] = pp[isbn_col].map(norm_isbn)\n",
    "    else:\n",
    "        pp2[\"ISBN\"] = \"\"  # we will fallback join on title\n",
    "\n",
    "    if title_col:\n",
    "        pp2[\"Title_pp\"] = pp[title_col].map(clean_text)\n",
    "    else:\n",
    "        pp2[\"Title_pp\"] = \"\"\n",
    "\n",
    "    pp2[\"Category\"] = pp[cat_col].map(clean_text) if cat_col else \"\"\n",
    "    pp2[\"Language\"] = pp[lang_col].map(clean_text) if lang_col else \"\"\n",
    "    pp2[\"Summary\"]  = pp[sum_col].map(clean_text) if sum_col else \"\"\n",
    "    pp = pp2\n",
    "\n",
    "# -----------------------\n",
    "# Merge Books + Side-info\n",
    "# -----------------------\n",
    "books_small = books[[\"ISBN\",\"Book-Title\",\"Book-Author\",\"Year-Of-Publication\",\"Publisher\"]].copy()\n",
    "books_small.rename(columns={\n",
    "    \"Book-Title\":\"Title\",\n",
    "    \"Book-Author\":\"Author\",\n",
    "    \"Year-Of-Publication\":\"Year\",\n",
    "}, inplace=True)\n",
    "\n",
    "# 1) Try ISBN join\n",
    "merged = pd.merge(books_small, pp, on=\"ISBN\", how=\"left\")\n",
    "\n",
    "# 2) For rows still missing Category/Language/Summary, try title fallback\n",
    "mask_missing = merged[\"Category\"].isna() | (merged[\"Category\"]==\"\")\n",
    "if \"Title_pp\" in merged.columns:\n",
    "    # build a small map from Title_pp to side-info\n",
    "    pp_title_map = pp[pp[\"Title_pp\"]!=\"\"][[\"Title_pp\",\"Category\",\"Language\",\"Summary\"]].drop_duplicates()\n",
    "    # prepare merge on title (case-insensitive normalized)\n",
    "    merged[\"Title_norm\"] = merged[\"Title\"].fillna(\"\").str.strip().str.lower()\n",
    "    pp_title_map[\"Title_norm\"] = pp_title_map[\"Title_pp\"].str.strip().str.lower()\n",
    "\n",
    "    merged = pd.merge(\n",
    "        merged.drop(columns=[\"Category\",\"Language\",\"Summary\"]), \n",
    "        pp_title_map[[\"Title_norm\",\"Category\",\"Language\",\"Summary\"]],\n",
    "        on=\"Title_norm\", how=\"left\"\n",
    "    ).drop(columns=[\"Title_norm\"])\n",
    "\n",
    "# Fill NaNs with blanks\n",
    "for c in [\"Category\",\"Language\",\"Summary\"]:\n",
    "    if c in merged.columns:\n",
    "        merged[c] = merged[c].fillna(\"\")\n",
    "    else:\n",
    "        merged[c] = \"\"\n",
    "\n",
    "# -----------------------\n",
    "# Interaction Matrix (CSR)\n",
    "# -----------------------\n",
    "print(\"[INFO] Building user/book encoders and CSR interactions...\")\n",
    "# Filter clearly invalid ISBNs\n",
    "valid_isbn_mask = merged[\"ISBN\"].astype(str).str.len() > 0\n",
    "merged_valid_isbn = set(merged.loc[valid_isbn_mask, \"ISBN\"].astype(str).tolist())\n",
    "\n",
    "ratings = ratings[ratings[\"ISBN\"].isin(merged_valid_isbn)].copy()\n",
    "if ratings.empty:\n",
    "    raise SystemExit(\"No ratings remain after filtering by valid ISBNs from books. Check data consistency.\")\n",
    "\n",
    "# Encoders\n",
    "users_unique = ratings[\"User-ID\"].astype(str).unique().tolist()\n",
    "books_unique = ratings[\"ISBN\"].astype(str).unique().tolist()\n",
    "\n",
    "user2idx = {u:i for i,u in enumerate(users_unique)}\n",
    "isbn2idx = {b:i for i,b in enumerate(books_unique)}\n",
    "\n",
    "# CSR components\n",
    "rows = ratings[\"User-ID\"].map(user2idx).to_numpy(np.int64)\n",
    "cols = ratings[\"ISBN\"].map(isbn2idx).to_numpy(np.int64)\n",
    "data = ratings[\"Book-Rating\"].astype(np.float32).to_numpy()\n",
    "\n",
    "n_users = len(users_unique)\n",
    "n_items = len(books_unique)\n",
    "R = csr_matrix((data, (rows, cols)), shape=(n_users, n_items))\n",
    "\n",
    "# Also build a binarized version (explicit >= 6 as \"liked\")\n",
    "data_bin = (ratings[\"Book-Rating\"].to_numpy() >= 6).astype(np.float32)\n",
    "R_bin = csr_matrix((data_bin, (rows, cols)), shape=(n_users, n_items))\n",
    "\n",
    "# -----------------------\n",
    "# Book metadata aligned to interactions\n",
    "# -----------------------\n",
    "meta = merged[merged[\"ISBN\"].isin(books_unique)].copy()\n",
    "meta = meta.drop_duplicates(subset=[\"ISBN\"], keep=\"first\")\n",
    "meta = meta.set_index(\"ISBN\").reindex(books_unique).reset_index()\n",
    "\n",
    "titles_arr  = meta[\"Title\"].fillna(\"\").astype(str).to_list()\n",
    "authors_arr = meta[\"Author\"].fillna(\"\").astype(str).to_list()\n",
    "years_arr   = meta[\"Year\"].fillna(0).astype(np.int32).to_list()\n",
    "pubs_arr    = meta[\"Publisher\"].fillna(\"\").astype(str).to_list()\n",
    "cats_arr    = meta[\"Category\"].fillna(\"\").astype(str).to_list()\n",
    "langs_arr   = meta[\"Language\"].fillna(\"\").astype(str).to_list()\n",
    "\n",
    "# -----------------------\n",
    "# Stats / Small samples\n",
    "# -----------------------\n",
    "n_ratings = ratings.shape[0]\n",
    "density = 100.0 * n_ratings / (n_users * n_items)\n",
    "\n",
    "user_activity = np.asarray(R.getnnz(axis=1)).astype(int)\n",
    "item_pop      = np.asarray(R.getnnz(axis=0)).astype(int)\n",
    "\n",
    "top_items_idx = np.argsort(-item_pop)[:20]\n",
    "top_items = [{\n",
    "    \"isbn\": books_unique[i],\n",
    "    \"title\": titles_arr[i],\n",
    "    \"author\": authors_arr[i],\n",
    "    \"ratings_count\": int(item_pop[i])\n",
    "} for i in top_items_idx]\n",
    "\n",
    "top_users_idx = np.argsort(-user_activity)[:20]\n",
    "top_users = [{\n",
    "    \"user_id\": users_unique[i],\n",
    "    \"ratings_count\": int(user_activity[i])\n",
    "} for i in top_users_idx]\n",
    "\n",
    "summary = {\n",
    "    \"n_users\": n_users,\n",
    "    \"n_items\": n_items,\n",
    "    \"n_ratings\": int(n_ratings),\n",
    "    \"matrix_density_percent\": float(round(density, 6)),\n",
    "    \"avg_ratings_per_user\": float(np.mean(user_activity)) if n_users>0 else 0.0,\n",
    "    \"avg_ratings_per_item\": float(np.mean(item_pop)) if n_items>0 else 0.0,\n",
    "    \"top_items_by_count\": top_items,\n",
    "    \"top_users_by_count\": top_users,\n",
    "}\n",
    "\n",
    "# -----------------------\n",
    "# SAVE: PKL (encoders, metadata, stats)\n",
    "# -----------------------\n",
    "artifact = {\n",
    "    \"user2idx\": user2idx,\n",
    "    \"isbn2idx\": isbn2idx,\n",
    "    \"users\": users_unique,\n",
    "    \"items_isbn\": books_unique,\n",
    "    \"items_title\": titles_arr,\n",
    "    \"items_author\": authors_arr,\n",
    "    \"items_year\": years_arr,\n",
    "    \"items_publisher\": pubs_arr,\n",
    "    \"items_category\": cats_arr,\n",
    "    \"items_language\": langs_arr,\n",
    "    \"summary\": summary,\n",
    "    \"source_paths\": {\n",
    "        \"preprocessed\": PP_PATH,\n",
    "        \"bx_books\": BX_BOOKS,\n",
    "        \"bx_ratings\": BX_RATINGS,\n",
    "        \"bx_users\": BX_USERS\n",
    "    },\n",
    "    \"meta\": {\n",
    "        \"python\": sys.version,\n",
    "        \"platform\": platform.platform(),\n",
    "        \"pandas\": pd.__version__,\n",
    "        \"numpy\": np.__version__\n",
    "    }\n",
    "}\n",
    "with open(PKL_OUT, \"wb\") as f:\n",
    "    pickle.dump(artifact, f)\n",
    "print(f\"[OK] Wrote PKL → {PKL_OUT}\")\n",
    "\n",
    "# -----------------------\n",
    "# SAVE: H5 (CSR + metadata arrays)\n",
    "# -----------------------\n",
    "with h5py.File(H5_OUT, \"w\") as hf:\n",
    "    dt = h5py.string_dtype(encoding=\"utf-8\")\n",
    "\n",
    "    # CSR for ratings (dense matrices are too big; store CSR pieces)\n",
    "    grp_r = hf.create_group(\"ratings\")\n",
    "    grp_r.create_dataset(\"data\",    data=R.data,    compression=\"gzip\")\n",
    "    grp_r.create_dataset(\"indices\", data=R.indices, compression=\"gzip\")\n",
    "    grp_r.create_dataset(\"indptr\",  data=R.indptr,  compression=\"gzip\")\n",
    "    grp_r.attrs[\"shape\"] = R.shape\n",
    "\n",
    "    grp_b = hf.create_group(\"ratings_binary\")\n",
    "    grp_b.create_dataset(\"data\",    data=R_bin.data,    compression=\"gzip\")\n",
    "    grp_b.create_dataset(\"indices\", data=R_bin.indices, compression=\"gzip\")\n",
    "    grp_b.create_dataset(\"indptr\",  data=R_bin.indptr,  compression=\"gzip\")\n",
    "    grp_b.attrs[\"shape\"] = R_bin.shape\n",
    "\n",
    "    # Aligned metadata arrays (index-aligned with items/books_unique)\n",
    "    hf.create_dataset(\"items_isbn\",      data=np.array(books_unique, dtype=object), dtype=dt)\n",
    "    hf.create_dataset(\"items_title\",     data=np.array(titles_arr,   dtype=object), dtype=dt)\n",
    "    hf.create_dataset(\"items_author\",    data=np.array(authors_arr,  dtype=object), dtype=dt)\n",
    "    hf.create_dataset(\"items_publisher\", data=np.array(pubs_arr,     dtype=object), dtype=dt)\n",
    "    hf.create_dataset(\"items_category\",  data=np.array(cats_arr,     dtype=object), dtype=dt)\n",
    "    hf.create_dataset(\"items_language\",  data=np.array(langs_arr,    dtype=object), dtype=dt)\n",
    "    hf.create_dataset(\"items_year\",      data=np.array(years_arr,    dtype=np.int32))\n",
    "\n",
    "    # Users (ID strings aligned with rows)\n",
    "    hf.create_dataset(\"users\", data=np.array(users_unique, dtype=object), dtype=dt)\n",
    "\n",
    "    # Misc\n",
    "    hf.attrs[\"created_unix\"] = int(time.time())\n",
    "    hf.attrs[\"notes\"] = \"CSR interactions + aligned metadata for Book-Crossing.\"\n",
    "\n",
    "print(f\"[OK] Wrote H5  → {H5_OUT}\")\n",
    "\n",
    "# -----------------------\n",
    "# SAVE: YAML (config/schema)\n",
    "# -----------------------\n",
    "cfg = {\n",
    "    \"project\": \"AI Book Recommender with Mood Detection\",\n",
    "    \"artifacts\": {\n",
    "        \"pkl\": PKL_OUT,\n",
    "        \"h5\":  H5_OUT,\n",
    "        \"yaml\": YAML_OUT,\n",
    "        \"json\": JSON_OUT\n",
    "    },\n",
    "    \"inputs\": {\n",
    "        \"preprocessed_csv\": PP_PATH,\n",
    "        \"bx_books_csv\": BX_BOOKS,\n",
    "        \"bx_ratings_csv\": BX_RATINGS,\n",
    "        \"bx_users_csv\": BX_USERS\n",
    "    },\n",
    "    \"schema\": {\n",
    "        \"ratings\": {\n",
    "            \"csr_groups\": [\"ratings\", \"ratings_binary\"],\n",
    "            \"shape\": [int(n_users), int(n_items)],\n",
    "            \"rating_scale\": \"0-10 (0=implicit); binary>=6 considered liked\"\n",
    "        },\n",
    "        \"book_meta_alignment\": \"All item arrays are index-aligned with items_isbn\",\n",
    "        \"user_indexing\": \"Row i in CSR corresponds to users[i]\"\n",
    "    },\n",
    "    \"build_meta\": {\n",
    "        \"python\": sys.version,\n",
    "        \"platform\": platform.platform(),\n",
    "        \"pandas\": pd.__version__,\n",
    "        \"numpy\": np.__version__\n",
    "    }\n",
    "}\n",
    "with open(YAML_OUT, \"w\", encoding=\"utf-8\") as f:\n",
    "    yaml.dump(cfg, f, sort_keys=False)\n",
    "print(f\"[OK] Wrote YAML → {YAML_OUT}\")\n",
    "\n",
    "# -----------------------\n",
    "# SAVE: JSON (summary + samples)\n",
    "# -----------------------\n",
    "sample_items = top_items[:5]\n",
    "sample_users = top_users[:5]\n",
    "with open(JSON_OUT, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump({\n",
    "        \"summary\": summary,\n",
    "        \"sample_top_items\": sample_items,\n",
    "        \"sample_top_users\": sample_users\n",
    "    }, f, ensure_ascii=False, indent=2)\n",
    "print(f\"[OK] Wrote JSON → {JSON_OUT}\")\n",
    "\n",
    "print(\"\\nAll artifacts created successfully ✅\")\n",
    "print(f\"Users: {n_users:,} | Items: {n_items:,} | Ratings: {n_ratings:,} | Density: {summary['matrix_density_percent']:.6f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb8de65-29a9-41f4-bdfa-09afc9b50779",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 (moviepy)",
   "language": "python",
   "name": "py311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
